{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "587dc2cc",
   "metadata": {},
   "source": [
    "# Datapipe - зачем и как\n",
    "Datapipe это инструмент для инкрементальной обработки данных. Интуитивно идея проста - при наличии некоторого количества табличных (или представимых в виде таблицы, например папка с фалйами) данных, мы хотим отслеживать и обрабатывать изменения в них по частям.  \n",
    "Мы рассмотрим простой пример: пользователи сайта генерирую события, из которых нужно получать и хранить информацию о пользователях. \n",
    "Обычно задача решается записью событий в таблицу, после чего происходит парсинг всех данных по расписанию. С datapipe нам  \n",
    "  \n",
    "а) не нужно каждый раз обрабатывать весь массив данных и  \n",
    "б) мы можем получать изменения во всех производных от основной таблицах в реальном времени. \n",
    "\n",
    "Пример будет очень простой: пользователь с \\<user_id\\> и \\<ip\\> сделал клик на сайте. В таблице events мы будем записывать каждое событие на сайте, агрегировать клики для каждого пользователя в таблице click_count, хранить и обновлять айпи адреса всех пользователей в таблице ip.\n",
    "  \n",
    "Сначала мы разберемся из чего состоит datapipe (он же труба), потом как туда попадают, и как редактируются данные. \n",
    "  \n",
    "*\n",
    "*Если вы не пользовались блокнотами - каждая ячейка выполняет блок кода внутри, переменные, полученные из разных ячеек, хранятся в общей памяти. Просто нажимайте shift + enter в каждой ячейке по порядку, и все будет работать.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224c3fc",
   "metadata": {},
   "source": [
    "Каждый пайплайн в трубе состоит из трех главных сущностей:\n",
    "- Catalog\n",
    "- Datastore\n",
    "- Pipeline  \n",
    "  \n",
    "Разберем по порядку:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658c28c",
   "metadata": {},
   "source": [
    "# Catalog\n",
    "В Сatalog описаны все таблицы, входящие в наш пайплайн. Каждую таблицу мы будем называть Store. Store бывают разные. Каждый умеет читать и записывать данные в свой тип хранилища. Оцените красоту идеи - в пайплайне могут одновременно находиться папки с файлами, таблицы в базе данных, файлы excel, и все это будет работать как одна система.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f3e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# ! pip install git+https://github.com/epoch8/datapipe.git\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412646e",
   "metadata": {},
   "source": [
    "Для каждой таблицы в каталоге нам нужно описать схему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6860b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, JSON, Integer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8afe072",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_schema = [\n",
    "    Column(\"user_id\", String(), primary_key=True),\n",
    "    Column(\"event_id\", String(), primary_key=True),\n",
    "    Column(\"event\", JSON())\n",
    "]\n",
    "click_count_schema = [\n",
    "    Column('user_id', String(), primary_key=True),\n",
    "    Column('click_count', Integer())\n",
    "]\n",
    "ip_schema = [\n",
    "    Column('user_id', String(), primary_key=True),\n",
    "    Column('ip', String())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d149c",
   "metadata": {},
   "source": [
    "Из соображений лаконичности все таблицы будут все лежать в одной БД, но как мы уже упоминали, это необязательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443f847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm store.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371765d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.store.database import DBConn\n",
    "dbconn = DBConn(\"sqlite:///store.sqlite\")\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "engine = create_engine(\"sqlite:///store.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77192434",
   "metadata": {},
   "source": [
    "Теперь можно обьявлять каталог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb92635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.compute import Catalog, Table\n",
    "from datapipe.store.database import TableStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b9bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_store = TableStoreDB(\n",
    "    name='events',                  # имя, с которым таблица будет храниться в БД\n",
    "    dbconn=dbconn,                  # соединение с бд, в которой будет храниться таблица\n",
    "    data_sql_schema=events_schema   # схема таблицы\n",
    ")\n",
    "\n",
    "click_store = TableStoreDB(\n",
    "    name='click_count',\n",
    "    dbconn=dbconn,\n",
    "    data_sql_schema=click_count_schema\n",
    ")\n",
    "\n",
    "ip_store = TableStoreDB(\n",
    "    name='ip',\n",
    "    dbconn=dbconn,\n",
    "    data_sql_schema=ip_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ccf1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = Catalog({\n",
    "    'events': Table(store=events_store),  # по этому имени мы будем обращаться к таблице в каталоге\n",
    "    'click_count': Table(store=click_store),\n",
    "    'ip': Table(store=ip_store)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476a4b5",
   "metadata": {},
   "source": [
    "- Catalog    - готово\n",
    "- Datastore\n",
    "- Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bfedb",
   "metadata": {},
   "source": [
    "# Datastore  \n",
    "Datastore нужен для хранения и отслеживания изменений в каталоге. Если в исходной таблице пайплайна поменяется какая-то строка, datastore будет знать, какая именно строка изменилась. Он позволяет не обрабатывать каждый раз весь массив данных во всех таблицах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5615954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.compute import DataStore\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a8ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataStore(dbconn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bc349",
   "metadata": {},
   "source": [
    "- Catalog    - готово\n",
    "- Datastore  - готово\n",
    "- Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf779e",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "Мы подошли к главному - pipeline описывает связи между таблицами в каталоге.  \n",
    "Переход от одной таблицы к другой, сопровождающийся каким-то преобразованием данных, мы называем Step.  \n",
    "Таким образом пайплайн состоит из Шагов, у каждого из которых есть входящие и исходящие таблицы.  \n",
    "Тут важно обратить внимание на то, что у каждого шага может быть больше одной входящей и исходящей таблицы. В datapipe можно строить сложные графы обработки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37356105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clicks_ip_step(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    res = []\n",
    "\n",
    "    res_ips = []\n",
    "\n",
    "    for user_id, grp in df.groupby(\"user_id\"):\n",
    "        events = grp['event'].apply(json.loads)\n",
    "        click_count = sum([1 for x in events if x['event_type'] == 'click'])\n",
    "        res.append({'user_id': user_id, 'click_count': click_count})\n",
    "        res_ips.append({'user_id': user_id, 'ip': events.iloc[-1]['ip']})\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame.from_records(res),\n",
    "        pd.DataFrame.from_records(res_ips),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65550782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.compute import Pipeline, build_compute\n",
    "from datapipe.core_steps import BatchTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ddcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    BatchTransform(                                   # Шаг между таблицами описывается сущностью BatchTransform\n",
    "        parse_clicks_ip_step,\n",
    "        inputs=[\"events\"],\n",
    "        outputs=[\"click_count\", \"ip\"],\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1741a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_compute собирает граф вычислений для всех описанных нами выше сущностей.\n",
    "steps = build_compute(ds, catalog, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd24e9",
   "metadata": {},
   "source": [
    "- Catalog    - готово\n",
    "- Datastore  - готово\n",
    "- Pipeline   - готово"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed78079",
   "metadata": {},
   "source": [
    "# API и обновление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2453759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.testclient import TestClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5add3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b65f7",
   "metadata": {},
   "source": [
    "В нашем АПИ всего один метод. Он получает на вход событие вида:  \n",
    " \n",
    "\"user_id\": ...,  \n",
    "    \"event_id\": ...,  \n",
    "    \"event\": {\"event_type\": \"click\", \"ip\": \"...\"}       \n",
    "    \n",
    "и при его получении запускает пересчет пайплайна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4684846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipe.types import ChangeList\n",
    "from datapipe.compute import run_steps_changelist\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class UpdateDataRequest(BaseModel):\n",
    "    user_id: str\n",
    "    event_id: str\n",
    "    event: dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82f220fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/update-data\")\n",
    "def update_data(req: UpdateDataRequest):\n",
    "    # Мы можем обращаться к таблицам в каталоге напрямую. \n",
    "    dt = catalog.get_datatable(ds, 'events')\n",
    "    \n",
    "    # в ChangeList мы будем хранить индексы строк,\n",
    "    # измененных или добавленных в исходную таблицу пайплайна  \n",
    "    cl = ChangeList()\n",
    "\n",
    "    # с помощью метода .store_chunk мы запишем входящие данные\n",
    "    # в исходную таблицу пайплайна. Если \n",
    "    chunk = pd.DataFrame({\n",
    "        'user_id': req.user_id,\n",
    "        'event_id': req.event_id,\n",
    "        'event': json.dumps(req.event)\n",
    "    }, index=[0])\n",
    "    idx = dt.store_chunk(chunk)\n",
    "\n",
    "    # Сделанные нами изменения запишем в ChangeList\n",
    "    cl.append('events', idx)\n",
    "\n",
    "    # Запустим пропагацию изменений по всем таблицам в пайплайне\n",
    "    run_steps_changelist(ds, steps, cl)\n",
    "\n",
    "    return {\n",
    "        \"result\": \"ok\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27abc51",
   "metadata": {},
   "source": [
    "Попробуем добавить события в пайплайн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c5cfb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_event_1 = {\n",
    "            \"user_id\": 1,\n",
    "            \"event_id\": 1,\n",
    "            \"event\": {\n",
    "                \"event_type\": \"click\",\n",
    "                \"ip\": \"111.111.111.111\",\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "650dba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': 'ok'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = TestClient(app)\n",
    "item = client.post(\"/update-data\", json=user_event_1).json()\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a9198",
   "metadata": {},
   "source": [
    "Убедимся, что в таблицы добавились данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821e4d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id event_id                                             event\n",
      "0       1        1  {\"event_type\": \"click\", \"ip\": \"111.111.111.111\"}\n",
      "  user_id  click_count\n",
      "0       1            1\n",
      "  user_id               ip\n",
      "0       1  111.111.111.111\n"
     ]
    }
   ],
   "source": [
    "events_dt = catalog.get_datatable(ds, 'events')\n",
    "print(events_dt.get_data())\n",
    "click_count_dt = catalog.get_datatable(ds, 'click_count')\n",
    "print(click_count_dt.get_data())\n",
    "ip_dt = catalog.get_datatable(ds, 'ip')\n",
    "print(ip_dt.get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2665b",
   "metadata": {},
   "source": [
    "Теперь добавим еще одно событие для этого же пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b628f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "user_event_2 = {\n",
    "            \"user_id\": 1,\n",
    "            \"event_id\": 2,\n",
    "            \"event\": {\n",
    "                \"event_type\": \"click\",\n",
    "                \"ip\": \"222.222.222.222\",\n",
    "            }\n",
    "        }\n",
    "item = client.post(\"/update-data\", json=user_event_2).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881d6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id event_id                                             event\n",
      "0       1        1  {\"event_type\": \"click\", \"ip\": \"111.111.111.111\"}\n",
      "1       1        2  {\"event_type\": \"click\", \"ip\": \"222.222.222.222\"}\n",
      "  user_id  click_count\n",
      "0       1            2\n",
      "  user_id               ip\n",
      "0       1  222.222.222.222\n"
     ]
    }
   ],
   "source": [
    "events_dt = catalog.get_datatable(ds, 'events')\n",
    "print(events_dt.get_data())\n",
    "click_count_dt = catalog.get_datatable(ds, 'click_count')\n",
    "print(click_count_dt.get_data())\n",
    "ip_dt = catalog.get_datatable(ds, 'ip')\n",
    "print(ip_dt.get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeeb93e",
   "metadata": {},
   "source": [
    "Работает! В таблице events сохранились оба уникальных события, таблица click_count увеличила счетчик, в таблице ip обновился ip адрес пользователя, который изменился в последнем запросе. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23846bf2",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как пайплайн обработает событие, в котором оба ключа уже имеются в наших таблицах.  \n",
    "Предположим, что мы нам не гарантированы уникальные события, например мы хотим переписать данные в имеющихся ключах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82acc6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "user_event_3 = {\n",
    "            \"user_id\": 1,\n",
    "            \"event_id\": 1,\n",
    "            \"event\": {\n",
    "                \"event_type\": \"clack\",\n",
    "                \"ip\": \"333.333.333.333\",\n",
    "            }\n",
    "        }\n",
    "item = client.post(\"/update-data\", json=user_event_3).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9264b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id event_id                                             event\n",
      "0       1        2  {\"event_type\": \"click\", \"ip\": \"222.222.222.222\"}\n",
      "1       1        1  {\"event_type\": \"clack\", \"ip\": \"333.333.333.333\"}\n",
      "  user_id  click_count\n",
      "0       1            1\n",
      "  user_id               ip\n",
      "0       1  333.333.333.333\n"
     ]
    }
   ],
   "source": [
    "events_dt = catalog.get_datatable(ds, 'events')\n",
    "print(events_dt.get_data())\n",
    "click_count_dt = catalog.get_datatable(ds, 'click_count')\n",
    "print(click_count_dt.get_data())\n",
    "ip_dt = catalog.get_datatable(ds, 'ip')\n",
    "print(ip_dt.get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac98419",
   "metadata": {},
   "source": [
    "Событие, которое хранилось в таблице по полученным ключам, обновилось вместе со всеми производными от него таблицами!\n",
    "Важно заметить, что последнее записанное нами в пайплайн событие оказалось в конце таблицы.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
